# Eating Action Detection System
A MATLAB application which takes sensor data and distinguishes eating actions from non-eating actions

### MOTIVATION
Machine Learning and Artificial Intelligence can be applied to almost every process in healthcare. Healthcare organizations are working with software companies to develop applications that are growingly digitized and automated. Scientists and researchers are using machine learning to create smart solutions to monitor, diagnose and treat various illness. 
Health monitoring systems are most beneficial as the device prevents the user from falling ill.  With the connectivity and agility provided by mobile and recent churn in IoT devices, building wearable devices and tracking them on user devices like mobile has become a reality. Data collected from monitoring systems can provide tremendous insights and open doors for research studies. User can view his health data on his phone or any electronic device and the doctor can view the health information regarding the user while accessing the patient’s disease. Now there is a need to identify the scenarios where monitoring devices can help, although we have seen a predominant work in cardiac diseases, sleep patterns, it still must address a lot of unanswered and unattempted issues.  Obesity or over eating is one of the biggest drivers of preventable chronic diseases and healthcare costs. 

With this inspiration we built a MATLAB application which takes sensor data and distinguishes eating actions from non-eating actions, which can be integrated into a mobile application to be programmed as a health monitoring device. In order to evaluate the performance prediction three different machine learning based techniques, namely, artificial neural network (ANN), support vector machine (SVM) and Decision Trees (DT) have been compared.

### PROJECT DESCRIPTION
The project was divided into four phases during the entire semester. During the first phase of the project, we had to generate the data and annotate the data. 
In second phase, after completing the data generation, data was annotated to mark eating and non-eating actions to train the classifier in later stages. After annotation, data from different sources, namely video and sensor data are interpolated to same time scale.  We generated 18 features and a label field which marks the truth whether a record is an eating or non-eating action.
In third phase we explored the sensor data and read the literature to determine which transformation can be applied to generate the new features and enable distinguishing eating actions from non-eating actions. We cleaned the data to fix any human annotation errors introduced in phase two. We applied eight transformations functions to each of the 18 features and plotted each of the transformed feature against eating action label to see which transformation on which feature produced huge variation between eating and non-eating actions. A thorough analysis of the transformations and features is discussed through and finally a list of 12 transformed features were carefully chosen. Since the feature size was still quite large for the model with only 2 decisions to make, we applied dimensionality reduction algorithm like PCA to reduce the dimension to 3 dimensions.
This phase taught use the importance of feature selection and the amount of time invested in making right feature selection decision as the model’s performance completely depends on the feature set chosen.
In fourth phase, we revisited the feature selection and verified the correctness of the features we chose and applied supervised classification algorithms like Support Vector Machines (SVM), Decision Tree (DT) and Artificial Neural Network (ANN) models. We evaluated the results by running several iterations and different feature sets. The accuracy report of various iterations is captured. A final report was generated with detailed explanation why such features were selected and why the transformation was applied. Results and recommendation of best algorithm was provided. 

###	Feature Creation and Selection

In this phase, we first normalized all the sensor data between scale of 0 and 1. Then we interpolated sensor data on to video data frame as video data was annotated to represent all features on same scale. MATLAB function interp1 was used for the same. The MATLAB function interp1 is as follows 
yi = interp1(x,Y,xi,method)
It interpolates to find yi, the values of the underlying function Y at the points in the vector or array xi, where x must be a vector. The interpolation can be performed using any method like Nearest Neighbour Interpolation, Linear Interpolation, Cubic Spline Interpolation etc. The method that is being used in the assignment is Cubic Spline Interpolation. The argument that is passed to the function is ‘Spline’.
We finally had 18 features which represented: Orientation X-W, Accelerometer X-Y, Gyroscope X-Z and EMG 1-8 (Electromyograph sensor). Since we never worked on sensor data, we read through some research papers which have used sensor data previously to perform classification. After thorough research we decided to use following transformation function on the data: (Mean, Standard Deviation, Median, Root Mean Square, Entropy, Fast Fourier Transform on sensor data)
Root Mean Square value of EMG sensor data represents the square root of average power of the EMG signal. It represents the time domain variable and is used to determine the degree of activation and force produced by the muscle. In our case force is produced by the muscle when user lifts his hand/wrist to perform the eating action.
Mean and median of EMG sensor represents the rate at which a muscle fatigues. In our case muscle relaxes after completing an eating action and the gap between another eating action is performed (for example, to chew the food)
Root mean square on accelerometer data is used to distinguish the walking patterns, we hoped it could distinguish eating actions, but it did not distinguish well as the variance in movement was less. 
Since wavelet transform captures sudden change in signals, it cannot be directly used to distinguish features, hence we calculated the energy of Fast Fourier transform as one of the transformations. 
We plotted each of the feature transformation using truth labels to view if the feature can distinguish the two actions. An example of plot is shown in figure 1. We finally chose following feature transformations: (RMS on emg 1,3,6,8, mean of orientation W and Z and standard deviation of X and Y, Power of emg 2,3,5,7)
Principal Component Analysis was applied on this set of 12 features and we selected 3 components which represented 97% of the data samples. Plot of PCA and the data it covers in shown in figure 2.

